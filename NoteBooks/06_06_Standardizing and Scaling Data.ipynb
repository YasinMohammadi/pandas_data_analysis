{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Standardizing and Scaling Data\n",
    "\n",
    "Standardizing and scaling are key preprocessing steps, especially for machine learning models, where differences in feature scales can impact model performance. These techniques transform data to a uniform scale without distorting differences in ranges or distributions.\n",
    "\n",
    "## Why Standardize?\n",
    "\n",
    "### Importance in Machine Learning Models\n",
    "1. **Improved Model Performance**:\n",
    "   - Many algorithms, such as support vector machines (SVMs) and k-nearest neighbors (KNN), are sensitive to feature scales. Standardization ensures all features contribute equally to the model.\n",
    "\n",
    "2. **Faster Convergence**:\n",
    "   - Gradient-based algorithms like logistic regression and neural networks converge faster with standardized data.\n",
    "\n",
    "3. **Uniform Feature Contribution**:\n",
    "   - Features with larger ranges don't dominate models unnecessarily.\n",
    "\n",
    "## Methods\n",
    "\n",
    "### 1. Min-Max Scaling\n",
    "Min-Max Scaling transforms data to a fixed range, typically [0, 1].\n",
    "\n",
    "#### Formula:\n",
    "$$ X_{scaled} = \\frac{X - X_{min}}{X_{max} - X_{min}} $$\n",
    "- **Use Case**: Suitable for algorithms that don't assume any distribution, such as KNN or linear regression.\n",
    "\n",
    "#### Example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Example DataFrame\n",
    "data = {'Feature': [10, 20, 30, 40, 50]}\n",
    "df = pd.DataFrame(data)\n",
    "print('Original DataFrame:')\n",
    "print(df)\n",
    "\n",
    "# Apply Min-Max Scaling\n",
    "scaler = MinMaxScaler()\n",
    "df['Feature_Scaled'] = scaler.fit_transform(df[['Feature']])\n",
    "print('DataFrame after Min-Max Scaling:')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Standard Scaling (Z-Score)\n",
    "\n",
    "Standard Scaling (or Z-Score Normalization) transforms data to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "#### Formula:\n",
    "$$ Z = \\frac{X - \\mu}{\\sigma} $$\n",
    "- **Use Case**: Suitable for algorithms that assume normally distributed data, such as logistic regression and principal component analysis (PCA).\n",
    "\n",
    "#### Example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Apply Standard Scaling\n",
    "scaler = StandardScaler()\n",
    "df['Feature_Standardized'] = scaler.fit_transform(df[['Feature']])\n",
    "print('DataFrame after Standard Scaling:')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- **Min-Max Scaling** scales data to a fixed range, typically [0, 1].\n",
    "- **Standard Scaling (Z-Score)** normalizes data to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "Both techniques are essential for ensuring consistent feature scaling, improving model performance, and achieving faster convergence in machine learning workflows."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
