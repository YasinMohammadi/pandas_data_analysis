{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b864183f",
   "metadata": {},
   "source": [
    "# 5. Importing and Exporting Data\n",
    "\n",
    "In this notebook, we will explore how to import data from various sources (CSV, Excel, JSON, SQL, APIs, and binary formats) into Pandas DataFrames and export data to different formats. We will also discuss customizing import/export options and handling compression.\n",
    "\n",
    "## Topics Covered:\n",
    "- Reading data from various formats: CSV, Excel, JSON, Parquet, SQL, HTML, APIs, Binary formats (Feather, ORC, HDF5)\n",
    "- Customizing import/export options\n",
    "- Handling compression (gzip, bz2, etc.)\n",
    "- Practical: Import/export with various formats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ff8b7a",
   "metadata": {},
   "source": [
    "## Reading Data from CSV\n",
    "\n",
    "Comma-Separated Values (CSV) files are one of the most common formats for tabular data.\n",
    "We will use the COVID-19 dataset from Indonesia as an example.\n",
    "\n",
    "### Steps:\n",
    "1. Use the `read_csv` function from Pandas.\n",
    "2. Specify the file path.\n",
    "3. Use parameters like `delimiter`, `header`, and `index_col` as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dda639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reading data from a CSV file\n",
    "csv_file = '../DataSets/Data_COVID19_Indonesia.csv'\n",
    "try:\n",
    "    covid_data = pd.read_csv(csv_file)\n",
    "    print(covid_data.head())\n",
    "except FileNotFoundError:\n",
    "    print(f'Error: The file {csv_file} does not exist. Please check the file path.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb2f98f",
   "metadata": {},
   "source": [
    "## Reading Data from Excel\n",
    "\n",
    "Excel files are often used for sharing structured data. Pandas provides the `read_excel` function to work with Excel files.\n",
    "\n",
    "We will use the IMF Investment and Capital Stock dataset as an example.\n",
    "\n",
    "### Steps:\n",
    "1. Use the `read_excel` function from Pandas.\n",
    "2. Specify the sheet name using the `sheet_name` parameter.\n",
    "3. Check for missing values and column types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b33b615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from an Excel file\n",
    "excel_file = '../DataSets/IMFInvestmentandCapitalStockDataset2021.xlsx'\n",
    "try:\n",
    "    imf_data = pd.read_excel(excel_file, sheet_name='Datasets')\n",
    "    print(imf_data.head())\n",
    "except FileNotFoundError:\n",
    "    print(f'Error: The file {excel_file} does not exist. Please check the file path.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b09763b",
   "metadata": {},
   "source": [
    "## Reading Data from JSON\n",
    "\n",
    "JSON (JavaScript Object Notation) is widely used for APIs and web data. Pandas can easily handle JSON files using the `read_json` function.\n",
    "\n",
    "We will use a mock signup dataset in JSON format as an example.\n",
    "\n",
    "### Steps:\n",
    "1. Use the `read_json` function from Pandas.\n",
    "2. Specify the file path.\n",
    "3. Use parameters like `orient` for nested JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f653bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from a JSON file\n",
    "json_file = '../DataSets/users.JSON'\n",
    "try:\n",
    "    json_data = pd.read_json(json_file)\n",
    "    print(json_data.head())\n",
    "except ValueError:\n",
    "    print(f'Error: Unable to parse JSON file {json_file}. Check the file content.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2d0f69",
   "metadata": {},
   "source": [
    "## Reading Data from SQL\n",
    "\n",
    "Structured Query Language (SQL) databases are widely used for storing large datasets. Pandas provides the `read_sql_query` function to query databases.\n",
    "\n",
    "We will use a mock signup dataset stored in an SQL database as an example.\n",
    "\n",
    "### Steps:\n",
    "1. Use the `sqlite3` library to connect to the database.\n",
    "2. Write a SQL query to fetch data.\n",
    "3. Use the `read_sql_query` function to load the data into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d77cb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from an SQL database\n",
    "import sqlite3\n",
    "\n",
    "# Connect to the database\n",
    "sql_file = '../DataSets/mock_signup.db'\n",
    "try:\n",
    "    connection = sqlite3.connect(sql_file)\n",
    "    sql_query = 'SELECT * FROM mock_signup'\n",
    "    sql_data = pd.read_sql_query(sql_query, connection)\n",
    "    print(sql_data.head())\n",
    "except sqlite3.DatabaseError:\n",
    "    print(f'Error: Unable to connect to the database {sql_file}. Check the file and query.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaae476f",
   "metadata": {},
   "source": [
    "## Reading Data from APIs\n",
    "\n",
    "APIs (Application Programming Interfaces) provide dynamic data access over the web. We use the `requests` library to fetch data and convert it into a DataFrame.\n",
    "\n",
    "### Steps:\n",
    "1. Use the `requests.get` function to fetch JSON data from an API.\n",
    "2. Convert the JSON response into a Pandas DataFrame using `pd.json_normalize()`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d5e635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Fetch data from a sample API\n",
    "api_url = 'https://jsonplaceholder.typicode.com/posts'\n",
    "response = requests.get(api_url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    api_data = response.json()\n",
    "    df_api = pd.json_normalize(api_data)\n",
    "    print('Data fetched from API:')\n",
    "    print(df_api.head())\n",
    "else:\n",
    "    print(f'Failed to fetch data. HTTP Status Code: {response.status_code}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42221ef",
   "metadata": {},
   "source": [
    "## Reading and Writing Binary Formats\n",
    "\n",
    "Binary file formats such as Feather, ORC, and HDF5 are optimized for fast reading and writing. Pandas supports these formats for efficient data storage and retrieval.\n",
    "\n",
    "### Steps:\n",
    "1. Use functions like `to_feather`, `to_parquet`, and `to_hdf` to write binary files.\n",
    "2. Use corresponding `read_` functions like `read_feather`, `read_parquet`, and `read_hdf` to load them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37955f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing and reading a Feather file\n",
    "feather_file = '../tmp/data.feather'\n",
    "df_api.to_feather(feather_file)\n",
    "print(f'Data written to Feather format at {feather_file}')\n",
    "\n",
    "# Reading a Feather file\n",
    "df_feather = pd.read_feather(feather_file)\n",
    "print('Data read from Feather format:')\n",
    "print(df_feather.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe3fbb1",
   "metadata": {},
   "source": [
    "## Exporting Data to CSV, Excel, and Other Formats\n",
    "\n",
    "Pandas makes it easy to export data to different formats using methods like `to_csv` and `to_excel`.\n",
    "\n",
    "### Steps:\n",
    "1. Use the appropriate method based on the desired format.\n",
    "2. Specify the file path and optional parameters like `index`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce97579d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting data to a CSV file\n",
    "output_csv_file = '../tmp/exported_covid_data.csv'\n",
    "covid_data.to_csv(output_csv_file, index=False)\n",
    "print(f'Data exported to {output_csv_file}')\n",
    "\n",
    "# Exporting data to an Excel file\n",
    "output_excel_file = '../tmp/exported_imf_data.xlsx'\n",
    "imf_data.to_excel(output_excel_file, index=False)\n",
    "print(f'Data exported to {output_excel_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d70c96",
   "metadata": {},
   "source": [
    "## Handling File Paths and Common Errors\n",
    "\n",
    "When working with files, common errors include:\n",
    "\n",
    "- **FileNotFoundError**: Ensure the file path is correct. Use absolute paths if needed.\n",
    "- **UnsupportedFormatError**: Verify the file format is supported by Pandas.\n",
    "- **PermissionError**: Check if you have write permissions for the directory when exporting files.\n",
    "- **ValueError**: Ensure the data format matches the expected input for functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0621f65",
   "metadata": {},
   "source": [
    "## Handling Compression\n",
    "\n",
    "Compression reduces file size for storage and transmission. Pandas supports compression formats like gzip, bz2, and zip for both import and export.\n",
    "\n",
    "### Example:\n",
    "- Reading a compressed file using `read_csv` with the `compression` parameter.\n",
    "- Exporting a DataFrame to a compressed file using `to_csv` with `compression`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1276afc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading compressed data\n",
    "compressed_input = '../DataSets/compressed_data.csv.gz'\n",
    "try:\n",
    "    compressed_data = pd.read_csv(compressed_input, compression='gzip')\n",
    "    print('Data read from compressed file:')\n",
    "    print(compressed_data.head())\n",
    "except FileNotFoundError:\n",
    "    print(f'Error: The file {compressed_input} does not exist.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c707faae",
   "metadata": {},
   "source": [
    "## Customizing Import and Export Options\n",
    "\n",
    "You can customize import/export options to handle specific requirements, such as setting delimiters, encoding, column selection, or compression.\n",
    "\n",
    "### Example:\n",
    "- Setting a custom delimiter while reading CSV files using the `sep` parameter.\n",
    "- Exporting data with specific encoding (e.g., UTF-8).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e01c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customizing import options\n",
    "csv_file = '../DataSets/custom_delimiter_data.csv'\n",
    "try:\n",
    "    custom_data = pd.read_csv(csv_file, sep=';', encoding='utf-8')\n",
    "    print('Data read with custom delimiter:')\n",
    "    print(custom_data.head())\n",
    "except FileNotFoundError:\n",
    "    print(f'Error: The file {csv_file} does not exist.')\n",
    "\n",
    "# Exporting data with compression\n",
    "compressed_file = '../tmp/compressed_data.csv.gz'\n",
    "df_api.to_csv(compressed_file, index=False, compression='gzip')\n",
    "print(f'Data exported with gzip compression to {compressed_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4172fc",
   "metadata": {},
   "source": [
    "## Practical: Reading and Saving Datasets\n",
    "\n",
    "Letâ€™s combine all the knowledge to read a dataset, filter data, and export it in various formats, including compressed formats.\n",
    "\n",
    "### Steps:\n",
    "1. Read a CSV file using `read_csv`.\n",
    "2. Filter rows based on specific conditions.\n",
    "3. Export the filtered data to multiple formats (CSV, Feather, gzip).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6147d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical example\n",
    "# Filter COVID data for provinces with more than 10,000 total cases\n",
    "filtered_data = covid_data[covid_data['Total Cases'] > 10000]\n",
    "\n",
    "# Export filtered data to CSV\n",
    "filtered_csv = '../tmp/filtered_covid_data.csv'\n",
    "filtered_data.to_csv(filtered_csv, index=False)\n",
    "print(f'Filtered data exported to CSV at {filtered_csv}')\n",
    "\n",
    "# Export filtered data to Feather format\n",
    "filtered_feather = '../tmp/filtered_covid_data.feather'\n",
    "filtered_data.to_feather(filtered_feather)\n",
    "print(f'Filtered data exported to Feather format at {filtered_feather}')\n",
    "\n",
    "# Export filtered data to compressed CSV\n",
    "filtered_compressed = '../tmp/filtered_covid_data.csv.gz'\n",
    "filtered_data.to_csv(filtered_compressed, index=False, compression='gzip')\n",
    "print(f'Filtered data exported with compression to {filtered_compressed}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LexLenz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
